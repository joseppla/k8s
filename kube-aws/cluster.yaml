# El nombre del cluster que le hemos pasado por parámetro.
clusterName: k8s-edyo
#
# # El nombre fqdn que le hemos pasado por parámetro.
externalDNSName: k8s-edyo.kubeops.net
#
# # Descomentamos este, la rama de la release de la versión de CoreOS 
# #(el SO que usaremos para deployar los nodos).
releaseChannel: stable
#
# # Descomentamos y ponemos a true para que la herramienta nos cree la entrada DNS en Route53.
createRecordSet: true
#
# # Descomentamos y ponemos el ZoneID de route53 del dominio anterior.
hostedZoneId: "Z2DLD0VCL3UZZN"
#
# # Descomentamos y ponemos el nombre de una llave SSH existente en AWS.
keyName: 'josep-amazon'
#
# # La zona que hemos pasado por paràmetro
region: us-east-1
#
# # La AZ que hemos pasado por paràmetro, la comentamos para obtener HA.
 #availabilityZone: us-east-1a
#
# # El ARN de la key KMS de AWS, es una clave que podemos generar en el panel de IAM de AWS y nos servirá 
# # para encriptar todas las comunicaciones entre masters, nodos y etcd, descomentamos y ponemos un ARN 
# # existente o generamos uno nuevo.
kmsKeyArn: "arn:aws:kms:us-east-1:012842039152:key/1d388002-974d-4bab-881d-3030ab07c7bf"
#
# # Descomentamos este, el número de masters que queremos, como vamos a deployar solo en 2 
# # zonas, lo pondremos a 2 aunqué podríamos poner tranquilamente 4
controllerCount: 2
#
# # Descomentamos este, es el tipo de instancia que vamos a usar para los masters, no pongais nada mas pequeño
# # que el default o no arrancara el cluster, para nuestro propósito pondremos m3.medium.
controllerInstanceType: m3.medium
#
# # Descomentamos este, es el número de workers que queremos en el cluster de Kubernetes, los nodos que se 
# #encargaran de arrancar los servicios que creemos nosotros, lo pondremos a 4 para tener 2 por zona.
# # Fix de última hora, este parámetro ya no es válido y lo sustituimos por #workers.nodePools[].count = 4
worker:
  nodePools:
  - name: nodepool1
    count: 4
#
# # Descomentamos este, el tipo de instancia de los workers, según lo que queramos deployar ha de ser mayor o 
# # menor, para nuestro proposito pondremos las mismas que en los masters pero según lo que queramos han de ser
# # de mayor o menor tamaño (para cosas mas serias mirar los pools de maquinas donde podremos especificar 
# # varios tipos).
workerInstanceType: m3.medium
#
# # Descomentamos este si quieremos que los workers tengan un tamaño de disco específico, no es aconsejable 
# # guardar datos en los discos de los nodos, mejor los subimos a 50G pero no los usaremos.
workerRootVolumeSize: 50
#
# # Descomentamos este, parámetro importante, el número de nodos en el cluster etcd, aquí subiremos de 1 a 3 
# # para tener alta disponibilidad, hay que tener en cuenta que kubernetes depende completamente de este 
# # cluster y a ser posible debería montarse por separado y tener algun sistema de auto-recovery.
etcdCount: 3
#
# # Descomentamos este, cambiaremos el tipo de instancia de los nodos de etcd a m3.medium para un mayor 
# # rendimiento aunque realmente para esta demo no lo necesitemos.
etcdInstanceType: m3.medium
#
# # Descomentamos este si queremos especificar el CIDR del VPC que se creará, par deployar en un VPC existente
# # leer el fichero original.
vpcCIDR: "10.0.0.0/16"
#
# # Descomentamos estos, aquí se pueden describir cuantas subnets queremos y de que tipo, públicas o privadas,
# # para esta demo usaremos dos públicas però las best practices de AWS no aconsejan este tipo de deployment.
subnets:  
  - name: ManagedPublicSubnet1
    private: false
    availabilityZone: us-east-1c
    instanceCIDR: "10.0.10.0/24"
  - name: ManagedPrivateSubnet1
    private: false
    availabilityZone: us-east-1e
    instanceCIDR: "10.0.20.0/24"
#
# Descomentamos este, es el tiempo que durará el certificado de los nodos , etcd, masters, mejor curarnos en
# salud y ponerlo igual que la CA a 10 años
tlsCertDurationDays: 3650

# Descomentamos este si queremos cambiar la versión del SO base, actualizaremos a la última versión estable.
kubernetesVersion: v1.5.3_coreos.0

# Descomentamos estos para poner tags al cluster y asi podelos diferenciar cuanto tengamos varios clusters, 
# realmente no es necesario si deployamos uno pero así vemos como queda todo
stackTags:  
  Name: "Kubernetes"
  Environment: "Edyo"
